#Logstash setup

#Can be setup using tar or using RPM

#Note
The Elastic Stack components are not available through the package manager by default, 
but you can install them with yum by adding Elastic’s package repository.
All of the Elastic Stack’s packages are signed with the Elasticsearch signing key in order to 
protect your system from package spoofing. Packages which have been authenticated 
using the key will be considered trusted by your package manager. 
In this step, you will import the Elasticsearch public GPG key and add the 
Elastic repository in order to install Elasticsearch.

#If using RPMs

#to download and install the Elasticsearch public signing key:
sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

#add the Elastic repository
sudo vi /etc/yum.repos.d/elasticsearch.repo

[elasticsearch-7.x]
name=Elasticsearch repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md

#if ES not already installed
sudo yum install elasticsearch

#& then editing (as per your path of installation)
sudo vi /etc/elasticsearch/config/elasticsearch.yml

#if Kibana not already installed
sudo yum install kibana

#& then editing (as per your path of installation)
vi /etc/kibana/config/kibana.yml 
------------------

LOGSTASH SETUP:
Two methods mentioned, either via RPMs or Tar (as per your method, follow the instructions)

#If using RPMs

#To install logstash
$sudo yum install logstash

#if enabling security
#using openssl to create certificates.

$cd /etc/logstash
$sudo mkdir ssl
#Generate SSL certificate. Change value of CN to your es-server name in the below command.(for example: n1 since ES is running on n1 as being one of the nodes of ES cluster)

$sudo openssl req -subj '/CN=n1/' -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout ssl/logstash-forwarder.key -out ssl/logstash-forwarder.crt

#The files explained below can be found in 'MyConfigsLS/conf.d/'
#Create following files inside “/etc/logstash/conf.d”

--These are just examples, actual files are in 'MyConfigsLS/conf.d/'

#if configuring logstash to receive data sent by a beat
#we need to create a input conf file

#sudo vim filebeat-input.conf
Add the following lines to it to configure input

input {
  beats {
    port => 5443
    type => syslog
    ssl => true
    ssl_certificate => "/etc/logstash/ssl/logstash-forwarder.crt"
    ssl_key => "/etc/logstash/ssl/logstash-forwarder.key"
  }
}

#Save and close the file 

#create a new configuration file for transformation(filter)

$sudo vim syslog-filter.conf
Add the following contents to it(here we are using GROK plugin) to filter/transform data

filter {
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }
}
#Save and exit the file. 

#Create output file to send data

$sudo vim output-elasticsearch.conf
Add the following lines to it to route output

output {
  elasticsearch { hosts => ["n1:9200"]
    hosts => "n1:9200"
    manage_template => false
    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
    document_type => "%{[@metadata][type]}"
  }
}

#if starting as a non-root user
#remember to
sudo chown -R elk:elk /var/lib/logstash
sudo chown -R elk:elk /var/log/logstash

#When we places these 3 files in /etc/logstash/conf.d and start logstash, we will have logstash take input from a beat,
filter it and push it to elasticsearch,

#to start logstash pipeline..
/usr/share/logstash/bin/logstash --path.settings /etc/logstash

#look for following output
#this is output from my machine where logstash was started.
[elk@n3 ~]$ /usr/share/logstash/bin/logstash --path.settings /etc/logstash
Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2020-11-16T22:27:48,023][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>"path.queue", :path=>"/var/lib/logstash/queue"}
[2020-11-16T22:27:48,043][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>"path.dead_letter_queue", :path=>"/var/lib/logstash/dead_letter_queue"}
[2020-11-16T22:27:48,536][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.8.13"}
[2020-11-16T22:27:48,567][INFO ][logstash.agent           ] No persistent UUID file found. Generating new UUID {:uuid=>"0cfdeb0b-e792-46f7-9dn3-9d26b4339a38", :path=>"/var/lib/logstash/uuid"}
[2020-11-16T22:27:59,067][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting "document_type" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>"document_type", :plugin=><LogStash::Outputs::ElasticSearch index=>"%{[@metadata][beat]}-%{+YYYY.MM.dd}", manage_template=>false, id=>"466f76dffb11c709b3cdaad13da660285839f9cfedc766a38f82492059568n3d", hosts=>[//n1:9200], document_type=>"%{[@metadata][type]}", enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_0334f39e-ebe5-440d-bfee-ceaab0e1e332", enable_metric=>true, charset=>"UTF-8">, workers=>1, template_name=>"logstash", template_overwrite=>false, doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, ilm_enabled=>false, ilm_rollover_alias=>"logstash", ilm_pattern=>"{now/d}-000001", ilm_policy=>"logstash-policy", action=>"index", ssl_certificate_verification=>true, selkfing=>false, selkfing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>}
[2020-11-16T22:27:59,398][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>2, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2020-11-16T22:28:00,587][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://n1:9200/]}}
[2020-11-16T22:28:00,825][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://n1:9200/"}
[2020-11-16T22:28:00,910][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2020-11-16T22:28:00,913][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2020-11-16T22:28:00,966][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//n1:9200"]}
[2020-11-16T22:28:01,608][INFO ][logstash.inputs.beats    ] Beats inputs: Starting input listener {:address=>"0.0.0.0:5443"}
[2020-11-16T22:28:01,651][INFO ][logstash.pipeline        ] Pipeline started successfully {:pipeline_id=>"main", :thread=>"#<Thread:0x7e9701e6 run>"}
[2020-11-16T22:28:01,729][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-11-16T22:28:01,798][INFO ][org.logstash.beats.Server] Starting server on port: 5443
[2020-11-16T22:28:02,463][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}

-----------------------------
#if using tar files
--Download logstash-7.8 from elastic.co

$cd /usr/local
$sudo tar -xvf /home/elk/Downloads/logstash-7.8.0.tar.gz
$sudo ln -s logstash-7.8.0 logstash
$sudo chown -R elk:elk logstash*
$cd
$vi .bashrc

--update .bashrc
export LOGSTASH_HOME=/usr/local/logstash
export PATH=$PATH:$LOGSTASH_HOME/bin

--refresh .bashrc
source .bashrc

$cd /usr/local/logstash
$mkdir conf.d

--copy the *.conf files from MyconfigsLS/conf.d/* into /usr/local/logstash/conf.d/

--start editing files as per your requirement

--to run logstash
logstash --path.settings /usr/local/logstash /usr/local/logstash/conf.d/logstash-xxxx.conf

--look into files in 'MyConfigsLS' for better understanding
