#Logstash setup
Note** change user 'elk' to 'hdu' or your specific user

Two methods mentioned, either via RPMs or Tar (as per your method, follow the instructions)
#Can be setup using tar or using RPM

#Note
The Elastic Stack components are not available through the package manager by default, 
but you can install them with yum by adding Elastic’s package repository.
All of the Elastic Stack’s packages are signed with the Elasticsearch signing key in order to 
protect your system from package spoofing. Packages which have been authenticated 
using the key will be considered trusted by your package manager. 
In this step, you will import the Elasticsearch public GPG key and add the 
Elastic repository in order to install Elasticsearch.

--if centos
#If using RPMs

#to download and install the Elasticsearch public signing key:
sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

#add the Elastic repository
sudo vi /etc/yum.repos.d/elasticsearch.repo

[elasticsearch-7.x]
name=Elasticsearch repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md

#if ES not already installed
sudo yum install elasticsearch

#& then editing (as per your path of installation)
sudo vi /etc/elasticsearch/config/elasticsearch.yml

#if Kibana not already installed
sudo yum install kibana

#& then editing (as per your path of installation)
vi /etc/kibana/config/kibana.yml 
------------------

#if using tar files
--Download logstash-7.8 from elastic.co
--Download logstash from https://opensearch.org/downloads.html

$cd /usr/local
$sudo tar -xvf /home/hdu/Downloads/logstash-7.8.0.tar.gz
OR
$sudo tar -xvf /home/hdu/Downloads/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz

$sudo ln -s logstash-7.8.0 logstash
OR
$sudo ln -s logstash-8.6.1 logstash

$sudo chown -R elk:elk logstash*
OR
$sudo chown -R hdu:hdu logstash*

$cd
$vi .bashrc

--update .bashrc
export LOGSTASH_HOME=/usr/local/logstash
export PATH=$PATH:$LOGSTASH_HOME/bin

--refresh .bashrc
source .bashrc

$cd /usr/local/logstash
$mkdir conf.d

--copy the *.conf files from MyconfigsLS/conf.d/* into /usr/local/logstash/conf.d/

--start editing files as per your requirement

--to run logstash
logstash --path.settings /usr/local/logstash -f /usr/local/logstash/conf.d/logstash_simple.conf

--look into files in 'MyConfigsLS' for better understanding

-----------
#If using RPMs

#To install logstash
$sudo yum install logstash

#if enabling security
#using openssl to create certificates.

$cd /etc/logstash
$sudo mkdir ssl
#Generate SSL certificate. Change value of CN to your es-server name in the below command.
(for example: c1 since ES is running on c1 as being one of the nodes of ES cluster)

$sudo openssl req -subj '/CN=c1/' -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout ssl/logstash-forwarder.key -out ssl/logstash-forwarder.crt

#Create following files inside “/etc/logstash/conf.d”

--These are just examples, actual files are in 'MyConfigsLS/conf.d/'

#if configuring logstash to receive data sent by a beat
#we need to create a input conf file

#sudo vim filebeat-input.conf
Add the following lines to it to configure input

input {
  beats {
    port => 5443
    type => syslog
    ssl => true
    ssl_certificate => "/etc/logstash/ssl/logstash-forwarder.crt"
    ssl_key => "/etc/logstash/ssl/logstash-forwarder.key"
  }
}

#Save and close the file 

#create a new configuration file for transformation(filter)

$sudo vim syslog-filter.conf
Add the following contents to it(here we are using GROK plugin) to filter/transform data

filter {
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }
}
#Save and exit the file. 

#Create output file to send data

$sudo vim output-elasticsearch.conf
Add the following lines to it to route output

output {
  elasticsearch { hosts => ["c1:9200"]
    hosts => "c1:9200"
    manage_template => false
    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
    document_type => "%{[@metadata][type]}"
  }
}

#if starting as a non-root user
#remember to
sudo chown -R elk:elk /var/lib/logstash
sudo chown -R elk:elk /var/log/logstash

#When we places these 3 files in /etc/logstash/conf.d and start logstash, we will have logstash take input from a beat,
filter it and push it to elasticsearch,

#to start logstash pipeline..
/usr/share/logstash/bin/logstash --path.settings /etc/logstash

#look for following output
#this is output from my machine where logstash was started.
[elk@n3 ~]$ /usr/share/logstash/bin/logstash --path.settings /etc/logstash
Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2022-11-16T22:28:01,798][INFO ][org.logstash.beats.Server] Starting server on port: 5443
[2022-11-16T22:28:02,463][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}

-----------------------------

