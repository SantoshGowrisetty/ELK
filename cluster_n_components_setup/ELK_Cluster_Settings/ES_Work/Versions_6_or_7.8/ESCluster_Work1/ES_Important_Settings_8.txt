Understanding Bootstraping & Discovery process
-----------------
Starting an Elasticsearch cluster for the very first time requires the initial set of 
master-eligible nodes to be explicitly defined on one or more of the master-eligible nodes 
in the cluster. This is known as cluster bootstrapping. 
This is only required the first time a cluster starts up: 
nodes that have already joined a cluster store this information in their data folder 
for use in a full cluster restart, and freshly-started nodes that are joining a 
running cluster obtain this information from the cluster’s elected master.

for fault tolerance it is better to bootstrap using at least three master-eligible nodes, 
each with a cluster.initial_master_nodes setting containing all three nodes.

cluster.initial_master_nodes:
  - master-a
  - master-b
  - master-c

or from command line
bin/elasticsearch -E cluster.initial_master_nodes=master-a,master-b,master-c

Note**
Once an Elasticsearch node has joined an existing cluster, or bootstrapped a new cluster, 
it will not join a different cluster. Elasticsearch will not merge separate clusters 
together after they have formed.

If you intended to add a node into an existing cluster but instead bootstrapped a separate 
single-node cluster then you must start again:

Shut down the node.
Completely wipe the node by deleting the contents of its data folder.
Configure discovery.seed_hosts or discovery.seed_providers and other relevant discovery 
settings.
Restart the node and verify that it joins the existing cluster rather than forming 
its own one-node cluster.

Discovery:
Discovery is the process by which the cluster formation module finds other nodes with which 
to form a cluster. This process runs when you start an Elasticsearch node or 
when a node believes the master node failed and continues until the master node is 
found or a new master node is elected.
--starts with a list of seed addresses

The process operates in two phases: First, each node probes the seed addresses by connecting 
to each address and attempting to identify the node to which it is connected and to verify 
that it is master-eligible. Secondly, if successful, it shares with the remote node a list 
of all of its known master-eligible peers and the remote node responds with its peers in turn. The node then probes all the new nodes that it just discovered, requests their peers, and so on.

If the node is not master-eligible then it continues this discovery process until it has 
discovered an elected master node. If no elected master is discovered then the node will
 retry after discovery.find_peers_interval which defaults to 1s.

--cluster formation module offers two seed hosts providers
settings-based 
file-based 

#file-based
discovery.seed_providers: file

--here file contains a list of hosts
Elasticsearch reloads this file when it changes, so that the list of seed nodes 
can change dynamically without needing to restart each node.

Voting configurations:
Each Elasticsearch cluster has a voting configuration, which is the set of master-eligible
 nodes whose responses are counted when making decisions such as electing a new master 
or committing a new cluster state.

After a node joins or leaves the cluster, Elasticsearch reacts by automatically making 
corresponding changes to the voting configuration in order to ensure that the cluster 
is as resilient as possible.

--The current voting configuration is stored in the cluster state
--Altering the voting configuration involves taking a vote, so it takes some time to 
adjust the configuration as nodes join or leave the cluster. 


GET /_cluster/state?filter_path=metadata.cluster_coordination.last_committed_config
--This list is limited in size by the cluster.max_voting_config_exclusions setting, 
which defaults to 10. 

--removing master-eligible nodes:
If there are only two master-eligible nodes remaining then neither node can be safely removed 
since both are required to reliably make progress. To remove one of these nodes you must first 
inform Elasticsearch that it should not be part of the voting configuration, and that the voting 
power should instead be given to the other node. You can then take the excluded node offline without
 preventing the other node from making progress. A node which is added to a voting configuration 
exclusion list still works normally, but Elasticsearch tries to remove it from the voting configuration
 so its vote is no longer required. Importantly, Elasticsearch will never automatically move 
a node on the voting exclusions list back into the voting configuration. Once an excluded node 
has been successfully auto-reconfigured out of the voting configuration, it is safe to shut it 
down without affecting the cluster’s master-level availability. A node can be added to the voting 
configuration exclusion list using the Voting configuration exclusions API.

# Add node to voting configuration exclusions list and wait for the system
# to auto-reconfigure the node out of the voting configuration up to the
# default timeout of 30 seconds
POST /_cluster/voting_config_exclusions?node_names=node_name

# Add node to voting configuration exclusions list and wait for
# auto-reconfiguration up to one minute
POST /_cluster/voting_config_exclusions?node_names=node_name&timeout=1m

Normally an exclusion is added when performing some maintenance on the cluster, 
and the exclusions should be cleaned up when the maintenance is complete. 
Clusters should have no voting configuration exclusions in normal operation.

If a node is excluded from the voting configuration because it is to be shut down permanently, 
its exclusion can be removed after it is shut down and removed from the cluster. 
Exclusions can also be cleared if they were created in error or were only required 
temporarily by specifying ?wait_for_removal=false.

# Wait for all the nodes with voting configuration exclusions to be removed from
# the cluster and then remove all the exclusions, allowing any node to return to
# the voting configuration in the future.
DELETE /_cluster/voting_config_exclusions

# Immediately remove all the voting configuration exclusions, allowing any node
# to return to the voting configuration in the future.
DELETE /_cluster/voting_config_exclusions?wait_for_removal=false







