#Setup & Configure filebeat (plugin) to send log lines to logstash/ES

The Filebeat client is a lightweight, resource-friendly tool that collects logs from files on 
the server and forwards these logs to your Logstash instance for processing.
Filebeat is designed for reliability and low latency.Filebeat has a light resource footprint
on the host machine, and the Beats input plugin minimizes the resource demands on the Logstash
instance.

##SETUP##
#If UBUNTU
#Download and install the Public Signing Key:
$sudo wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -

#Install “apt-transport-https” and add repo.
$sudo apt-get install apt-transport-https

$sudo echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list

#Update repo and install Filebeat
$sudo apt-get update
$sudo apt-get install filebeat

#If Centos
#to download and install the Elasticsearch public signing key:
sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

#add the Elastic repository
sudo vi /etc/yum.repos.d/elasticsearch.repo

--change version as per your requirement
[elasticsearch-7.x]
name=Elasticsearch repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md

then
yum install filebeat

CONFIGURATION
#default template for index name
#Option 1: Here index name would be default "filebeat-%{[agent.version]}-%{+yyyy.MM.dd}"

#Sending data directly to ES
#Modify Filebeat configurations.
$sudo vim /etc/filebeat/filebeat.yml

enabled: true
paths:
    - /var/log/messages*

#comment the following lines:(to avoid sending data to logstash)
#output.logstash:
  # The Logstash hosts
  #hosts: ["elk-server:5443"]

#Uncomment Elasticsearch:(to send data to ES)
output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["c1:9200"]

check if index was created..

------------
#Option 2: 
#Modify Filebeat configurations.
$sudo vim /etc/filebeat/filebeat.yml

enabled: true
paths:
    - /usr/local/elasticsearch/logs/mycluster.log

#comment the following lines:
#output.logstash:
  # The Logstash hosts
  #hosts: ["elk-server:5443"]

#set the index dynamically by using a format string to access any event field
#The events are distributed to these nodes in round robin order. If one node becomes unreachable, 
the event is automatically sent to another node. 
#Uncomment Elasticsearch:
output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["c1:9200","c2:9200","c3:9200"]
  
#setting our own index name, which would also mean if we need to use our own template 
#setup.ilm.enabled: false
#setup.template:
# settings:
#   index.number_of_shards: 3
#   name: "ESlog"
#   pattern: "ESlog-*"
# enabled: false 

output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["c1:9200"]
  index: "ESlog-%{[agent.version]}-%{+yyyy.MM.dd}" 

#other options to customize index and data landing into index
#if data contains field type:log_type ,that could be used to route data into 2 different indexes
  #With this configuration, all events with log_type: normal are sent to an index named normal-xxx and
                            all events with log_type: critical are sent to critical-xxx

    #- index: "%{[fields.log_type]}-%{[agent.version]}-%{+yyyy.MM.dd}"

#based on messages
    #- index: "warning-%{[agent.version]}-%{+yyyy.MM.dd}"
         when.contains:
           message: "WARN"
    #- index: "error-%{[agent.version]}-%{+yyyy.MM.dd}"
         when.contains:
           message: "ERR"

#based on types
    #- index: "%{[fields.log_type]}"
         mappings:
           critical: "sev1"
           normal: "sev2"
           default: "sev3"
 
check if index was created..

------------------------------
#Sending data to logstash to fwd to ES
#Modify Filebeat configurations.
$sudo vim /etc/filebeat/filebeat.yml

enabled: true
paths:
    - /var/log/elasticsearch/mycluster.log

#Uncomment the following lines:
output.logstash:
  # The Logstash hosts
  hosts: ["c4:5443"]

#Comment Elasticsearch:
#output.elasticsearch:
  # Array of hosts to connect to.
  # hosts: ["localhost:9200"]

--to be done after logstash is setup
#Enable filebeat on system boot OR Start filebeat service
$sudo systemctl enable filebeat.service
$sudo systemctl start filebeat.service

--to check status
$sudo service filebeat status

###on logstash running node#####
#Creating a sample pipeline on logstash node to accept beats from filebeat on client server
#To Test

$root@u1:~# vi /etc/logstash/conf.d/fb_pipeline.conf
input{
  beats {
        port => "5443"
        }
     }
output {
       stdout{codec => "rubydebug"}
       }

#Testing newly created config
$logstash --path.settings /usr/local/logstash/ -f /usr/local/logstash/conf.d/fb_pipeline.conf --config.test_and_exit
Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2020-11-26T22:31:44,420][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
Configuration OK
[2020-11-26T22:31:53,406][INFO ][logstash.runner          ] Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash

#To run logstash with above mentioned config, remember to update output location in 
*.conf to receive content sent by filebeat and forward it 
#as per location

--refer fb_pipeline.conf provided in >>>> Logstash_Work/MyConfigsLS/conf.d

#now start logstash with this config
$logstash --path.settings /usr/local/logstash/ -f /usr/local/logstash/conf.d/fb_pipeline.conf 

###on filebeat running node####
#start filebeat service 
or 
start command to publish data 

#if filebeat was installed as a tar, then run the command from appropriate location
root@u2:/usr/share/filebeat/bin# ./filebeat -e -c /usr/share/filebeat/filebeat.yml -d publish

#Note**
Filebeat consists of two main components: inputs and harvesters. These components work together to tail files and send event data to the output that you specify.
A harvester is responsible for reading the content of a single file. The harvester reads each file, line by line, and sends the content 
to the output. One harvester is started for each file. The harvester is responsible for opening and closing the file, which means that 
the file descriptor remains open while the harvester is running. If a file is removed or renamed while it’s being harvested, 
Filebeat continues to read the file. This has the side effect that the space on your disk is reserved until the harvester closes. 
By default, Filebeat keeps the file open until close_inactive is reached.

Filebeat keeps the state of each file and frequently flushes the state to disk in the registry file. The state is used to remember the 
last offset a harvester was reading from and to ensure all log lines are sent. If the output, such as Elasticsearch or Logstash, is not reachable, 
Filebeat keeps track of the last lines sent and will continue reading the files as soon as the output becomes available again. 
While Filebeat is running, the state information is also kept in memory for each input. When Filebeat is restarted, data from the 
registry file is used to rebuild the state, and Filebeat continues each harvester at the last known position.

For each input, Filebeat keeps a state of each file it finds. Because files can be renamed or moved, the filename and path are 
not enough to identify a file. For each file, Filebeat stores unique identifiers to detect whether a file was harvested previously. 

Filebeat guarantees that events will be delivered to the configured output at least once and with no data loss. 
Filebeat is able to achieve this behavior because it stores the delivery state of each event in the registry file.

##if filename is changed for new instance of filebeat and index pattern is changed, then we can continue to run filebeat from same location
OR
##we can clear the registry and metadata entry from <filebeat_location>/data folder by using command such as
rm -rf /<filebeatpath>/data/*
 
===========================
#check from Kibana if filebeat is sending data to logstash,if logstash is fwding same to Elasticsearch 
and a new index is created.

