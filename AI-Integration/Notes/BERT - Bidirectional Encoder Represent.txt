TensorFlow - is a machine learning framework is famous for powering DNN with high level code.
It is a library to work with Linear algebra and statictics.
Tensor describes a multi linear relationship between sets of algebraic objects within a vector space(multi-dimensional array).
It has collection of APIs for
-data processing
-visualization
-model evaluation
-deployment

It is extremely portable 
TensorFlow Lite- can run on tiny mobile CPUs or microcontrollers with TF Lite
TensorFlow JS - can run in browser 
Core library can scale up to multiple GPUs (CUDA) or TPUs (Tensor Processing Units)

Used in Medicine- for object detection (medical imaging)
Used by Twitter - Predictive analytics
Used by Spotigy - recommendation engines
Used by Paypal - fraud detection

Used in Self driving cars, NLP.

Example:
--create a py file : test.py
import tensorflow as tf
import numpy as np
print(tf.__version__)

<code>

Keraslayer:
TensorFlow Hub (tfhub) is a repository of pre-trained TensorFlow models that can be reused across various AI projects. One of the key components of tfhub is hub.KerasLayer, which allows you to seamlessly integrate these pre-trained models into your Keras applications. 

hub.KerasLayer is a wrapper that allows you to use a TensorFlow Hub model as a Keras layer. This means you can incorporate pre-trained models into your Keras applications with just a few lines of code. This can significantly speed up your model development process and improve the performance of your models.

Why Use hub.KerasLayer?
There are several reasons why you might want to use hub.KerasLayer:

Efficiency: You can leverage pre-trained models, which saves time and computational resources.
Performance: Pre-trained models have been trained on large datasets and can often outperform models trained from scratch.
Flexibility: hub.KerasLayer can be used with any TensorFlow Hub model, giving you a wide range of models to choose from.

How to Use hub.KerasLayer 
!pip install tensorflow tensorflow_hub
import tensorflow as tf
import tensorflow_hub as hub

we can use hub.KerasLayer to incorporate a pre-trained model into our Keras application. For example 
we can use the Inception V3 model, which is a popular model for image classification.

model_url = "https://tfhub.dev/google/imagenet/inception_v3/feature_vector/4"
hub_layer = hub.KerasLayer(model_url, trainable=False)

model_url is the URL of the pre-trained model on TensorFlow Hub. 
The trainable parameter is set to False to freeze the weights of the pre-trained model. 
If you want to fine-tune the pre-trained model, you can set trainable to True.

create a Keras model and add the hub.KerasLayer as the first layer.
model = tf.keras.Sequential([
    hub_layer,
    tf.keras.layers.Dense(1000, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

In this model, the hub.KerasLayer extracts features from the input images, and the subsequent dense layers perform classification based on these features.

compile and train the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

model.fit(train_data, train_labels, epochs=5)

--Now we can use this model.
----------------------------
Bert Usage & understanding:

Other options:
https://tfhub.dev/google/collections/bert/1

From jupyter or your setup
#Setup
pip install "tensorflow>=2.0.0"
pip install tensorflow-text

--------------------------------------------------------------------
#Example1
import tensorflow_hub as hub
import tensorflow_text as text

#we can download models or use links
preprocess_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'
encoder_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'

bert_preprocess_model = hub.KerasLayer(preprocess_url)

text_test = ['nide movie indeed', ' I love python programming']
text_preprocessed = bert_preprocess_model(text_test)
text_preprocessed.keys()

text_preprocessed['input_mask']

#CLS nice movie indeed SEP

text_preprocessed['input_type_ids']
text_preprocessed['input_word_ids']

bert_model = hub.KerasLayer(encoder_url)
bert_results = bert_model(text_preprocessed)
bert_results.keys()
---------------------------------------------------

#Classify email as spam or not spam

Six chances to win CASH! From 100 to 20,000 pounds txt (spam)
John , can we postpone our meeting to next Monday, please?(not spam)

Bert converts email content to an embedding vector.
This can be fed into a neural network and do the training.
[A vector is generated of 768 length.]

Within BERT we have two components
--preprocess
--encode

------------------------------
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text

import pandas as pd
from keras.backend import shape

df = pd.read_csv("/home/hdu/Downloads/spamtrain.csv")
df.head(5)

#basic analysis
df.groupby('label').describe()
#should show you count of type of emails
#we need to handle inbalancing of dataset

#Let's use technique for 'down sampling '
df['label'].value_counts()

#check for percentage of emails in data

#discard some emails to have equal number of spam and ham emails
#we can also do oversampling of smaller data as other option

df_spam = df[df['label'] == 'spam']
df_spam.shape
df_spam.head()
df_ham = df[df['label'] == 'ham']
df_ham.shape
df_ham.head()

df_ham_downsampled = df_ham.sample(df_spam.shape[0])
df_ham_downsampled.shape

#concat both
df_balanced = pd.concat([df_spam, df_ham_downsampled])
df_balanced.columns
df_balanced['label'].value_counts()
#should show equal number of spam and ham emails

df_balanced.sample(5)

df_balanced['spam'] = df_balanced['label'].apply(lambda x: 1 if x == 'spam' else 0)
df_balanced.columns
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_balanced['label'], df_balanced['spam'],
                                                    stratify=df_balanced['spam'])
#Note** use startify so that in train and test sample, the distribution of categories is equal.

X_train.head
df_balanced.head()
preprocess_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'
encoder_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'

bert_preprocess_model = hub.KerasLayer(preprocess_url)
bert_encoder = hub.KerasLayer(encoder_url)

#let's write a function to get embedding for a sentence

def get_sentence_embedding(sentences):
    preprocessed_text = bert_preprocess_model(sentences)
    return bert_encoder(preprocessed_text)['pooled_output']

get_sentence_embedding([
    "100$ discount, hurry up",
    "peter, are you available for a discussion tomorrow"])

#test bert encoder to get embeddings for some random words
e = get_sentence_embedding([
    "banana",
    "grapes",
    "mango",
    "jeff bezos",
    "elon musk",
    "bill gates"
])

from sklearn.metrics.pairwise import cosine_similarity

# we use consine similarity to see how similar two vectors are,
# if cosine simlarity is close to 1 that means vectors are similar

cosine_similarity([e[0]], [e[1]])

#similarly
cosine_similarity([e[0]], [e[3]])

# we can create sequential or functional model in Keras

# Bert layers
# create input layer
text_input = tf.keras.layers.Input(shape=(),dtype=tf.string, name="label")
preprocessed_text = bert_preprocess_model(text_input)
outputs = bert_encoder(preprocessed_text)

# Neural network layers
# dropout layer : refers to dropping out the nodes (input and hidden layer) in a neural network
l = tf.keras.layers.Dropout(0.1, name='dropout')(outputs['pooled_output'])
l = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(l)
#Note ** if more than 0.5 sigmoid = spam or not

# construct final model
model = tf.keras.Model(inputs=[text_input], outputs=[l])

model.summary()

#should show you 'Trainable params': 769(input: 768 + 1)
#remaining params come from BERT which are not trainable

METRICS = [
    tf.keras.metrics.BinaryAccuracy(name='accuracy'),
    tf.keras.metrics.Precision(name='precision'),
    tf.keras.metrics.Recall(name='recall')]

model.compile(optimizer='adam',
              loss='binary_crossentropy',
                metrics = METRICS)

model.fit(X_train, y_train, epochs=10)
# The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.

model.evaluate(X_test, y_test)

y_predicted = model.predict(X_test)
y_predicted = y_predicted.flatten()

import numpy as np

y_predicted = np.where(y_predicted > 0.5, 1, 0)
y_predicted

from sklearn.metrics import confusion_matrix, classification_report

cm = confusion_matrix(y_test, y_predicted)
cm

from matplotlib import pyplot as plt
import seaborn as sn

sn.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_predicted))
#should show good precision, recall and fl_score

reviews = [
    'Enter  a chance to win $5000, hurry up, offer valid until march 31,2011',
    'You are awarded a SiPix Digital Camera! call 00191928267 from landline. Delivery within 27days.',
    'it is 80488. Your 500 free text messages are valid until 31 December 2005',
    'Hey peter, are you coming to office today',
    "why don't you wait for 1 week to check your salary"]

model.predict(reviews)
# Look at values and see if its more than 0.5 to be categorized as SPAM.

-------------------------------
Fine Tuning:
import pandas as pd
df = pd.read_csv('/home/hdu/Downloads/spamtrain.csv')
df.head()
df.columns

#independent features
X = list(df['text'])
X
#dependent features
y = list(df['label'])
y

y = list(pd.get_dummies(y,drop_first=True)['spam'])
y

from sklearn.model_selection import train_test_split
X_train,X_test, y_train,y_test = train_test_split(X,y,test_size=0.20, random_state=0)

#pip install transformers
#call the pretrained model > call the tokenizer >
from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

#padding to make all sentences of same size
#truncation to remove white spaces
train_encodings = tokenizer(X_train, truncation=True, padding=True)
test_encodings = tokenizer(X_test, truncation=True, padding=True)

train_encodings
test_encodings

#convert encodings into dataset objects
import tensorflow as tf
train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),y_test))

from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments

training_args = TFTrainingArguments(
    output_dir = '/home/hdu/results',   #output dir
    num_train_epochs=2,        #total number of training epochs
    per_device_train_batch_size=8, #batch size per device during training
    per_device_eval_batch_size=16,  #batch size for evaluation
    warmup_steps=500,                #number of warmup steps for learning rate scheduler
    weight_decay=0.01,               #strength of weight decay
    logging_dir='/home/hdu/logs',           #dirctory for storing logs
    logging_steps=10,
)

with training_args.strategy.scope():
    model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

trainer = TFTrainer(
    model=model,         #the instantiated transformers model to be trained
    args=training_args,     #training arguments defined above
    train_dataset=train_dataset,                 #training_dataset
    eval_dataset=test_dataset,                   #evaluation dataset
)

trainer.train()

trainer.evaluate(test_dataset)

trainer.evaluate(test_dataset)
trainer.predict(test_dataset)
trainer.predict(test_dataset)[1].shape

output = trainer.evaluate(test_dataset)[1]

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,output)

cm

trainer.save_model('senti_model')

-------------------------------
NLP

--Automatic article summarization
--responsive chatbots
--creative writing generation

Transfer Learning
---------------
Training a computer vision or natural language model can be expensive.
--requires lot of data and in case of computer vision it needs to be labeled by humans and may take lot of time to train on expensive hardware.
For ex: GPT2  a benchmark setting language model released in 2019 by OpenAI , estimately costed $1.6m to train.

Now we can 
--download these models for free on internet( pretrained on enormous datasets) and ready to go.
--we can also fine-tune these models quickly to work on our datasets.
Training/Pre-training : Getting a whole new engine
Fine-tuning : taking car to a mechanic and getting new spark plugs.

This process of taking a model that’s been trained to do one task — as these pre-trained models have been trained to do — and then fine-tuning it to work on a related but different task is at the essence of what’s called TRANSFER LEARNING...

-----------------
After converting words into a numerical form machine learning models can understand, these are fed into the main part of the model which is often a deep, multi-layered neural network.
THe most popular language models at the moment , THE TRANSFORMER, have a structure where they build a very deep set of relationships between every word in the sentence,
creating a rich (numerical) representation of a sentence.
This rich representation is then fed into the last layer of the model which, given a part of the sentence, is trained to predict the next word.
It does this by giving its confidence of predicting what the next word is over all of the words in its vocabulary.

with Transfer Learning in NLP was that researchers discovered that when you train a model to predict the next word, you can take the trained model, chop off the layer that predicts the next word and put on a new layer and train just that last layer — very quickly — to predict the sentiment of a sentence. Now, keep in mind that this is not what the model was trained to do; it was trained to predict the next word in the sentence; however, it appears to be capturing a lot of pertinent information in a sentence when it processes and transforms into the rich representations which are fed into the last layer to predict the next word. But you can do more than just fine-tune the model to predict the sentiment of a sentence. You can fine-tune the model to be a conversational AI,or you can fine-tune the model to take in questions and pull the answers out of a piece of text relatively quickly.

Previously, language models were generally trained for very specific tasks and often trained from scratch which can be painful.

Transformer:
----------------- 
Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease.
It relies entirely on self-attention to compute representations of its input and output WITHOUT using sequence-aligned RNNs or convolution.

Understanding Attention:
Input sequence (details of objects, how objects are related)
----------------intermediary information captured from Input sequence as hown above--------( important and impacts output quality)
                [size of vector is fixed]
----expressed in output language------------
Note** with long sentences the intermediate state fails and is not sufficient to capture all information

Attention mechanism tries to overcome the information bottleneck of the intermediary state by allowing the decoder model to access all the hidden states
   Allows us to focus on parts of our input sequence while we predict our output sequence.

------------------

BERT (Bidirectional Encoder Representations from Transformers) 
--Applies the bidirectional training of transformer(a popular attention model) to attention model.
--This is in contrast to earlier efforts which looked at a text sequence either from left to right or combined left to right and right to left training.
--As per BERT , a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models.
--A novel technique 'Masked LM(MLM) which allows birectional training in models which was not possible earlier.

Bakground:
In the field of computer vision, researchers have repeatedly shown the value of transfer learning (pre-training a neural network model on a known task
, for example ImageNet and then performing fine tuning-using the trainedneural network as the basis of a new purpose specific model.
Similar technique can be useful in many natural language tasks.
A different approach which is also popular in NLP tasks and shown in ELMo paper, is feature-based training.
In this approach, a pre-trained neural network produces word embeddings which are then used as features in NLP models.

How BERT works:
It makes use of transformer , an attention mechanism that learns contextual relations between words(sub-words) in a text.
In its vanilla form, transformer includes 2 separate mechanisms 
--an encoder that reads the text input 
--a decoder that produces  a prediction for the task.
Since BERT's goal is to generate a language model, only encoder mechanism is necessary.


As opposed to directional models , which read the text input sequentially (left-right or right-left), the transformer encoder reads the entire sequence of
words at once.Thus it is considered birectional , but it would be more accurate to say its non-directional.
This characteristic allows the model to learn the context of a word based on all of its durroundings (left and right of word).

View the chart which is a high-level description of the Transformer encoder.
The input is a sequence of tokens, which are first embedded into vectors and then processed in the neural network,
The output is a sequence of vectors of size H, in which each vector correcponds to an input token with the same index.

When training language models, there is a challenge of defining prediction goal.
Many models predict the next word in a sequence (eg: the man came home from ____), a directional approach which inherently limits context learning.
To overcome this challenge, BERT uses two training strategies:

Masked LM (MLM)
-------------
Before feeding word sequences into BERT , 15% of the words in each sequence are replaced with a [MASK] token .
The model then attempts to predict the original value of the masked words, based on context provided by the other, non-masked , words in sequence.
In technical terms, prediction of the output words requires:

1. Adding a classification layer on top of the encoder output.
2. Multiplying the output vector by the emebedding matrix , transforming them into the vocabulary dimension.
3. Calculating the probability of each word in the vocabulary with softmax.

The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, a characteristic which is offset by its increased context awareness.

NSP-Next Sentence Prediction
In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.

To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:

A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.
A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.
A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.

To predict if the second sentence is indeed connected to the first, the following steps are performed:

The entire input sequence goes through the Transformer model.
The output of the [CLS] token is transformed into a 2×1 shaped vector, using a simple classification layer (learned matrices of weights and biases).
Calculating the probability of IsNextSequence with softmax.
When training the BERT model, Masked LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies.

How to use BERT(FINE-TUNING)
BERT can be used for a wide variety of language tasks, while only adding a small layer to the core model:

1. Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output for the [CLS] token.
2. In Question Answering tasks (e.g. SQuAD v1.1), the software receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.
3. In Named Entity Recognition (NER), the software receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label.

In the fine-tuning training, most hyper-parameters stay the same as in BERT training, and the paper gives specific guidance (Section 3.5) on the hyper-parameters that require tuning. The BERT team has used this technique to achieve state-of-the-art results on a wide variety of challenging natural language tasks.

tAKEAWAYS:
Model size matters, even at huge scale. BERT_large, with 345 million parameters, is the largest model of its kind. It is demonstrably superior on small-scale tasks to BERT_base, which uses the same architecture with “only” 110 million parameters.
With enough training data, more training steps == higher accuracy. For instance, on the MNLI task, the BERT_base accuracy improves by 1.0% when trained on 1M steps (128,000 words batch size) compared to 500K steps with the same batch size.
BERT’s bidirectional approach (MLM) converges slower than left-to-right approaches (because only 15% of words are predicted in each batch) but bidirectional training still outperforms left-to-right training after a small number of pre-training steps.

https://github.com/google-research/bert

Understand word masking
----------------
Training the language model in BERT is done by predicting 15% of the tokens in the input, that were randomly picked. These tokens are pre-processed as follows — 80% are replaced with a “[MASK]” token, 10% with a random word, and 10% use the original word. The intuition that led the authors to pick this approach is as follows (Thanks to Jacob Devlin from Google for the insight):

If we used [MASK] 100% of the time the model wouldn’t necessarily produce good token representations for non-masked words. The non-masked tokens were still used for context, but the model was optimized for predicting masked words.
If we used [MASK] 90% of the time and random words 10% of the time, this would teach the model that the observed word is never correct.
If we used [MASK] 90% of the time and kept the same word 10% of the time, then the model could just trivially copy the non-contextual embedding.

===============
Examples:

