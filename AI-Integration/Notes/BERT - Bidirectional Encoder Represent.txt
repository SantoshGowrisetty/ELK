NLP

--Automatic article summarization
--responsive chatbots
--creative writing generation

Transfer Learning
---------------
Training a computer vision or natural language model can be expensive.
--requires lot of data and in case of computer vision it needs to be labeled by humans and may take lot of time to train on expensive hardware.
For ex: GPT2  a benchmark setting language model released in 2019 by OpenAI , estimately costed $1.6m to train.

Now we can 
--download these models for free on internet( pretrained on enormous datasets) and ready to go.
--we can also fine-tune these models quickly to work on our datasets.
Training/Pre-training : Getting a whole new engine
Fine-tuning : taking car to a mechanic and getting new spark plugs.

This process of taking a model that’s been trained to do one task — as these pre-trained models have been trained to do — and then fine-tuning it to work on a related but different task is at the essence of what’s called TRANSFER LEARNING...

-----------------
After converting words into a numerical form machine learning models can understand, these are fed into the main part of the model which is often a deep, multi-layered neural network.
THe most popular language models at the moment , THE TRANSFORMER, have a structure where they build a very deep set of relationships between every word in the sentence,
creating a rich (numerical) representation of a sentence.
This rich representation is then fed into the last layer of the model which, given a part of the sentence, is trained to predict the next word.
It does this by giving its confidence of predicting what the next word is over all of the words in its vocabulary.

with Transfer Learning in NLP was that researchers discovered that when you train a model to predict the next word, you can take the trained model, chop off the layer that predicts the next word and put on a new layer and train just that last layer — very quickly — to predict the sentiment of a sentence. Now, keep in mind that this is not what the model was trained to do; it was trained to predict the next word in the sentence; however, it appears to be capturing a lot of pertinent information in a sentence when it processes and transforms into the rich representations which are fed into the last layer to predict the next word. But you can do more than just fine-tune the model to predict the sentiment of a sentence. You can fine-tune the model to be a conversational AI,or you can fine-tune the model to take in questions and pull the answers out of a piece of text relatively quickly.

Previously, language models were generally trained for very specific tasks and often trained from scratch which can be painful.

Transformer:
----------------- 
Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease.
It relies entirely on self-attention to compute representations of its input and output WITHOUT using sequence-aligned RNNs or convolution.

Understanding Attention:
Input sequence (details of objects, how objects are related)
----------------intermediary information captured from Input sequence as hown above--------( important and impacts output quality)
                [size of vector is fixed]
----expressed in output language------------
Note** with long sentences the intermediate state fails and is not sufficient to capture all information

Attention mechanism tries to overcome the information bottleneck of the intermediary state by allowing the decoder model to access all the hidden states
   Allows us to focus on parts of our input sequence while we predict our output sequence.

------------------

BERT (Bidirectional Encoder Representations from Transformers) 
--Applies the bidirectional training of transformer(a popular attention model) to attention model.
--This is in contrast to earlier efforts which looked at a text sequence either from left to right or combined left to right and right to left training.
--As per BERT , a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models.
--A novel technique 'Masked LM(MLM) which allows birectional training in models which was not possible earlier.

Bakground:
In the field of computer vision, researchers have repeatedly shown the value of transfer learning (pre-training a neural network model on a known task
, for example ImageNet and then performing fine tuning-using the trainedneural network as the basis of a new purpose specific model.
Similar technique can be useful in many natural language tasks.
A different approach which is also popular in NLP tasks and shown in ELMo paper, is feature-based training.
In this approach, a pre-trained neural network produces word embeddings which are then used as features in NLP models.

How BERT works:
It makes use of transformer , an attention mechanism that learns contextual relations between words(sub-words) in a text.
In its vanilla form, transformer includes 2 separate mechanisms 
--an encoder that reads the text input 
--a decoder that produces  a prediction for the task.
Since BERT's goal is to generate a language model, only encoder mechanism is necessary.


As opposed to directional models , which read the text input sequentially (left-right or right-left), the transformer encoder reads the entire sequence of
words at once.Thus it is considered birectional , but it would be more accurate to say its non-directional.
This characteristic allows the model to learn the context of a word based on all of its durroundings (left and right of word).

View the chart which is a high-level description of the Transformer encoder.
The input is a sequence of tokens, which are first embedded into vectors and then processed in the neural network,
The output is a sequence of vectors of size H, in which each vector correcponds to an input token with the same index.

When training language models, there is a challenge of defining prediction goal.
Many models predict the next word in a sequence (eg: the man came home from ____), a directional approach which inherently limits context learning.
To overcome this challenge, BERT uses two training strategies:

Masked LM (MLM)
-------------
Before feeding word sequences into BERT , 15% of the words in each sequence are replaced with a [MASK] token .
The model then attempts to predict the original value of the masked words, based on context provided by the other, non-masked , words in sequence.
In technical terms, prediction of the output words requires:

1. Adding a classification layer on top of the encoder output.
2. Multiplying the output vector by the emebedding matrix , transforming them into the vocabulary dimension.
3. Calculating the probability of each word in the vocabulary with softmax.

The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, a characteristic which is offset by its increased context awareness.

NSP-Next Sentence Prediction
In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.

To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:

A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.
A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.
A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.

To predict if the second sentence is indeed connected to the first, the following steps are performed:

The entire input sequence goes through the Transformer model.
The output of the [CLS] token is transformed into a 2×1 shaped vector, using a simple classification layer (learned matrices of weights and biases).
Calculating the probability of IsNextSequence with softmax.
When training the BERT model, Masked LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies.

How to use BERT(FINE-TUNING)
BERT can be used for a wide variety of language tasks, while only adding a small layer to the core model:

1. Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output for the [CLS] token.
2. In Question Answering tasks (e.g. SQuAD v1.1), the software receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.
3. In Named Entity Recognition (NER), the software receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label.

In the fine-tuning training, most hyper-parameters stay the same as in BERT training, and the paper gives specific guidance (Section 3.5) on the hyper-parameters that require tuning. The BERT team has used this technique to achieve state-of-the-art results on a wide variety of challenging natural language tasks.

tAKEAWAYS:
Model size matters, even at huge scale. BERT_large, with 345 million parameters, is the largest model of its kind. It is demonstrably superior on small-scale tasks to BERT_base, which uses the same architecture with “only” 110 million parameters.
With enough training data, more training steps == higher accuracy. For instance, on the MNLI task, the BERT_base accuracy improves by 1.0% when trained on 1M steps (128,000 words batch size) compared to 500K steps with the same batch size.
BERT’s bidirectional approach (MLM) converges slower than left-to-right approaches (because only 15% of words are predicted in each batch) but bidirectional training still outperforms left-to-right training after a small number of pre-training steps.

https://github.com/google-research/bert

Understand word masking
----------------
Training the language model in BERT is done by predicting 15% of the tokens in the input, that were randomly picked. These tokens are pre-processed as follows — 80% are replaced with a “[MASK]” token, 10% with a random word, and 10% use the original word. The intuition that led the authors to pick this approach is as follows (Thanks to Jacob Devlin from Google for the insight):

If we used [MASK] 100% of the time the model wouldn’t necessarily produce good token representations for non-masked words. The non-masked tokens were still used for context, but the model was optimized for predicting masked words.
If we used [MASK] 90% of the time and random words 10% of the time, this would teach the model that the observed word is never correct.
If we used [MASK] 90% of the time and kept the same word 10% of the time, then the model could just trivially copy the non-contextual embedding.


