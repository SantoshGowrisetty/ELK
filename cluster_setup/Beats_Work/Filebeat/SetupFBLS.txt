#Configure filebeat (plugin) to send log lines to logstash

The Filebeat client is a lightweight, resource-friendly tool that collects logs from files on 
the server and forwards these logs to your Logstash instance for processing.
Filebeat is designed for reliability and low latency.Filebeat has a light resource footprint
on the host machine, and the Beats input plugin minimizes the resource demands on the Logstash
instance.

SETUP

#If UBUNTU
#Download and install the Public Signing Key:
$sudo wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -

#Install “apt-transport-https” and add repo.
$sudo apt-get install apt-transport-https

$sudo echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list

#Update repo and install Filebeat
$sudo apt-get update
$sudo apt-get install filebeat

#If Centos
#to download and install the Elasticsearch public signing key:
sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

#add the Elastic repository
sudo vi /etc/yum.repos.d/elasticsearch.repo

[elasticsearch-6.x]
name=Elasticsearch repository for 6.x packages
baseurl=https://artifacts.elastic.co/packages/6.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md

then
yum install filebeat

CONFIGURATION

#Option 1: Here index name would be default "filebeat-%{[agent.version]}-%{+yyyy.MM.dd}"

#Sending data directly to ES
#Modify Filebeat configurations.
$sudo vim /etc/filebeat/filebeat.yml

enabled: true
paths:
    - /var/log/messages*

#comment the following lines:
#output.logstash:
  # The Logstash hosts
  #hosts: ["elk-server:5443"]
  #ssl.certificate_authorities: ["/etc/filebeat/logstash-forwarder.crt"]

#Uncomment Elasticsearch:
#output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["ce1:9200"]

check if index was created..

#Option 2: 
#Modify Filebeat configurations.
$sudo vim /etc/filebeat/filebeat.yml

enabled: true
paths:
    - /var/log/messages*

#comment the following lines:
#output.logstash:
  # The Logstash hosts
  #hosts: ["elk-server:5443"]
  #ssl.certificate_authorities: ["/etc/filebeat/logstash-forwarder.crt"]

#set the index dynamically by using a format string to access any event field
#With this configuration, all events with log_type: normal are sent to an index named normal-xxx and all events with log_type: critical are sent to critical-xxx
#The events are distributed to these nodes in round robin order. If one node becomes unreachable, the event is automatically sent to another node. 
#Uncomment Elasticsearch:
#output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["ce1:9200","ce2:9200","ce3:9200"]
  index: "%{[fields.log_type]}-%{[agent.version]}-%{+yyyy.MM.dd}"

check if index was created..

===========================================
#Sending data to logstash to fwd to ES
#Modify Filebeat configurations.
$sudo vim /etc/filebeat/filebeat.yml

enabled: true
paths:
    - /var/log/elasticsearch/mycluster.log

#Uncomment the following lines:
output.logstash:
  # The Logstash hosts
  hosts: ["elk-server:5443"]
  ssl.certificate_authorities: ["/etc/filebeat/logstash-forwarder.crt"]

#Comment Elasticsearch:
#output.elasticsearch:
  # Array of hosts to connect to.
  # hosts: ["localhost:9200"]

#Now go to logstash server and get “logstash-forwarder.crt” contents
$sudo cat /etc/logstash/ssl/logstash-forwarder.crt
#get the contents 

#create a certificate file on filebeat client server
$sudo vim /etc/filebeat/logstash-forwarder.crt
#copy the logstash-forwarder.crt contents here

#Enable filebeat on system boot Start filebeat service
$sudo systemctl enable filebeat.service
$sudo systemctl start filebeat.service
$sudo service filebeat status
$sudo service filebeat stop

#we will start it again after we setup logstash to accept beats..

###on logstash running node#####
#Creating a sample pipeline on logstash node to accept beats from filebeat on client server
#To Test

$root@u1:~# vi /etc/logstash/conf.d/first-pipeline.conf
input{
  beats {
        port => "5443"
        }
     }
output {
       stdout{codec => "rubydebug"}
       }

#Testing newly created config
root@u1:~# /usr/share/logstash/bin/logstash --path.settings /etc/logstash/ -f /etc/logstash/conf.d/first-pipeline.conf --config.test_and_exit

#To run logstash with above mentioned config, remember to update output location in *.conf to receive content sent by filebeat and forward it 
#as per location

input{
  beats {
        port => "5443"
        }
     }
output {
     elasticsearch {
          hosts => "192.168.56.104"
          index => "LogInfo"
          document_type => "AllLogs"
      }
      stdout {}

#now start logstash with this config
root@u1:~# /usr/share/logstash/bin/logstash --path.settings /etc/logstash/ -f /etc/logstash/conf.d/first-pipeline.conf

###on filebeat running node####
#to check
root@u2:/usr/share/filebeat/bin# ./filebeat -e -c /etc/filebeat/filebeat.yml -d "publish"

#if fine, start filebeat service
#start filebeat as a service or start filebeat from command line from /usr/share/bin/filebeat
===========================
#check from Kibana if filebeat is sending data to logstash,if logstash is fwding same to Elasticsearch 
and a new index is created.

