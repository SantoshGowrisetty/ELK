#using logstash to send csv file to ES and create index automatically
#Download dataset from "https://www.kaggle.com/mirosval/personal-cars-classifieds"
$mkdir data

$cd data

$unzip classified-ads-for-cars.zip

$mv classified-ads-for-cars.csv cars.csv

$vi logstash_cars.config

#paste this content 
input {
     file {
      path => "/root/data/cars.csv"
      start_position => "beginning"
      sincedb_path => "/dev/null"
      }
}
filter {
     csv {
         separator => ","
         columns => ["maker","model","mileage","manufacture_year","engine_displacement","engine_power","body_type","color_slug","stk_year","transmission","door_count","seat_count","fuel_type","date_created","date_last_seen","price_eur"]
     }
     mutate {convert => ["mileage","integer"]}
     mutate {convert => ["price_eur","float"]}
     mutate {convert => ["engine_power","integer"]}
     mutate {convert => ["door_count","integer"]}
     mutate {convert => ["seat_count","integer"]}
}
output {
     elasticsearch {
          hosts => "localhost"
          index => "cars"
          document_type => "sold_cars"
      }
      stdout {}
}

#note** Make sure elasticsearch & kibana service is running already


#Before starting logstash test if installation is fine,by giving below line & provide an input 'Hello world'
#note** sudo bin/logstash -e 'input {stdin {}} output {stdout{}}'

----sample run
root@u1:~# /usr/share/logstash/bin/logstash --path.settings /etc/logstash/ -e 'input {stdin {}} output {stdout{}}'
Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2019-09-19T23:34:23,705][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2019-09-19T23:34:23,725][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.8.3"}
[2019-09-19T23:34:30,422][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>2, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2019-09-19T23:34:30,560][INFO ][logstash.pipeline        ] Pipeline started successfully {:pipeline_id=>"main", :thread=>"#<Thread:0x5183cdc9 run>"}
The stdin plugin is now waiting for input:
[2019-09-19T23:34:30,693][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2019-09-19T23:34:31,017][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
hello world
/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/awesome_print-1.7.0/lib/awesome_print/formatters/base_formatter.rb:31: warning: constant ::Fixnum is deprecated
{
       "message" => "hello world",
    "@timestamp" => 2019-09-19T21:34:38.867Z,
      "@version" => "1",
          "host" => "u1"
}
----

#start logstash pointing to this config file
$/usr/share/logstash/bin/logstash --path.settings /etc/logstash/ -f /root/data/logstash_cars.config

#testing while data is being pushed into ES

17.
GET /cars

GET /cars/_search
{
  "query": {
     "match_all": {}
  }
}

GET /cars/_count

#once data is loaded and index is created

#Management>Index patterns> create index pattern for 'cars' index

#Test from Discover

#Use Visualization > Create a visualization
--choose pie-chart --choose index cars [click] --click on split slices --choose aggregations & 'Terms' --field : maker.keyword --Order By : metric:Count
--Size: 10 > click on play
#save the chart: Top Ten Vehicles Quantity
#create a dashboard and add your visualization & save
#Use Visualization > Create a visualization
--choose Data-table --choose index cars [click] --click on split rows --choose aggregations & 'Terms' --field : maker.keyword --Order By : metric:Count
--choose Data-table --choose index cars [click] --click on split rows --choose aggregations & 'Terms' --field : model.keyword --Order By : metric:Count
--Size: 5 > click on play
#save the chart: Top Five Manufacturers
==========================

#Configure filebeat (plugin) to send log lines to logstash
18.
The Filebeat client is a lightweight, resource-friendly tool that collects logs from files on 
the server and forwards these logs to your Logstash instance for processing.
Filebeat is designed for reliability and low latency.Filebeat has a light resource footprint
on the host machine, and the Beats input plugin minimizes the resource demands on the Logstash
instance.
#Download and install the Public Signing Key:
$sudo wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -

#Install “apt-transport-https” and add repo.
$sudo apt-get install apt-transport-https

$sudo echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list

#Update repo and install Filebeat
$sudo apt-get update
$sudo apt-get install filebeat

#Modify Filebeat configurations.
$sudo vim /etc/filebeat/filebeat.yml

enabled: true
paths:
    - /var/log/elasticsearch/mycluster.log

#Uncomment the following lines:
output.logstash:
  # The Logstash hosts
  hosts: ["elk-server:5443"]
  ssl.certificate_authorities: ["/etc/filebeat/logstash-forwarder.crt"]

#Comment Elasticsearch:
#output.elasticsearch:
  # Array of hosts to connect to.
  # hosts: ["localhost:9200"]

#Now go to logstash server and get “logstash-forwarder.crt” contents
$sudo cat /etc/logstash/ssl/logstash-forwarder.crt
#get the contents 

#create a certificate file on filebeat client server
$sudo vim /etc/filebeat/logstash-forwarder.crt
#copy the logstash-forwarder.crt contents here

#Enable filebeat on system boot Start filebeat service
$sudo systemctl enable filebeat.service
$sudo systemctl start filebeat.service
$sudo service filebeat status
$sudo service filebeat stop

#we will start it again after we setup logstash to accept beats..

###on logstash running node#####
#Creating a sample pipeline on logstash node to accept beats from filebeat on client server
#To Test

$root@u1:~# vi /etc/logstash/conf.d/first-pipeline.conf
input{
  beats {
        port => "5443"
        }
     }
output {
       stdout{codec => "rubydebug"}
       }

#Testing newly created config
root@u1:~# /usr/share/logstash/bin/logstash --path.settings /etc/logstash/ -f /etc/logstash/conf.d/first-pipeline.conf --config.test_and_exit

#To run logstash with above mentioned config, remember to update output location in *.conf to receive content sent by filebeat and forward it 
#as per location

input{
  beats {
        port => "5443"
        }
     }
output {
     elasticsearch {
          hosts => "192.168.56.104"
          index => "LogInfo"
          document_type => "AllLogs"
      }
      stdout {}

#now start logstash with this config
root@u1:~# /usr/share/logstash/bin/logstash --path.settings /etc/logstash/ -f /etc/logstash/conf.d/first-pipeline.conf

###on filebeat running node####
#to check
root@u2:/usr/share/filebeat/bin# ./filebeat -e -c /etc/filebeat/filebeat.yml -d "publish"

#if fine, start filebeat service
#start filebeat as a service or start filebeat from command line from /usr/share/bin/filebeat
===========================
#check from Kibana if filebeat is sending data to logstash,if logstash is fwding same to Elasticsearch and a new index is created.

